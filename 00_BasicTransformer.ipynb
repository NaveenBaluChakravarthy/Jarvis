{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Build_Jarvis.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"14gjsNLG6Ek1IX9P8uN3FqUiAtJwTJo-M","authorship_tag":"ABX9TyNTZXVyG42mrT66WWromMjN"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"EGdnPJJ-AC5_"},"source":["### Imports"]},{"cell_type":"code","metadata":{"id":"1v0VNupNQuvt"},"source":["import re\n","import os\n","import json\n","import spacy\n","import random\n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","import tensorflow_datasets as tfds"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SZcFPtqZAG8s"},"source":["### Reproducibility"]},{"cell_type":"code","metadata":{"id":"VHnpoL-5USFh"},"source":["C_Seed = 1\n","os.environ['PYTHONHASHSEED'] = str(C_Seed)\n","random.seed(C_Seed)\n","np.random.seed(C_Seed)\n","tf.random.set_seed(C_Seed)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"G2Zf0mePALbs"},"source":["### Initializations"]},{"cell_type":"code","metadata":{"id":"uBWP-dJqPzPy"},"source":["# Custom Parameters\n","C_FilePath = \"/content/drive/MyDrive/Jarvis/Data/MarvelCinematicUniverse_02_CaptainAmericaFirstAvenger.csv\"\n","C_max_convos = None\n","C_max_length = 40\n","C_batch_size = 64\n","C_num_layers = 2\n","C_num_units = 512\n","C_d_model = 256\n","C_num_heads = 8\n","C_activation = 'relu'\n","C_epochs = 40\n","C_dropout = 0.1\n","C_max_vocab_size = 2**15\n","C_Random_State = 0\n","C_actual_vocab_size = 16\n","C_nlp = spacy.load('en_core_web_sm')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TIBM7U_nXxpV"},"source":["# Utilities"]},{"cell_type":"code","metadata":{"id":"A7e-BUV-pOkc"},"source":["class DataUtilities:\n","    \"\"\"Getting the data, preprocess to remove / replace unsupported patterns and encode the data. \"\"\"\n","\n","    def __init__(self):\n","        \"\"\"Importing the data from a csv and initializing the variables used in the class. \"\"\"  \n","        self.data = pd.read_csv(C_FilePath)\n","        if C_max_convos:\n","            self.data = self.data[1:C_max_convos]\n","        self.tokenized_questions = []\n","        self.tokenized_answers = []\n","        self.tokenizer = None\n","        self.dataset = None\n","        self.questions = []\n","        self.answers = []\n","\n","    def preprocess(self, string):\n","        \"\"\"Cleaning the data and removing unsupported patterns from the input. \"\"\"\n","        string = string.lower()\n","        string = re.sub(r'[^a-z0-9]', ' ', string)\n","        string = re.sub(r'\\s{1,}', ' ', string)\n","        doc = C_nlp(string)\n","        string = [token.lemma_ for token in doc if token.text not in C_nlp.Defaults.stop_words]\n","        return ' '.join(string)\n","\n","    def tokenize(self):\n","        \"\"\"Tokenizing, encoding and padding to form the dataset for model training. \"\"\"\n","\n","        self.data['Q'] = self.data['Questions'].apply(self.preprocess)\n","        self.data['A'] = self.data['Answers'].apply(self.preprocess)\n","\n","        self.questions = self.data['Q'].tolist()\n","        self.answers = self.data['A'].tolist()\n","\n","        self.tokenizer = tf.keras.preprocessing.text.Tokenizer()\n","        self.tokenizer.fit_on_texts(self.questions + self.answers)\n","        C_actual_vocab_size = len(self.tokenizer.word_index)\n","\n","        self.tokenized_questions = self.tokenizer.texts_to_sequences(self.questions)\n","        self.tokenized_answers = self.tokenizer.texts_to_sequences(self.answers)\n","\n","        self.tokenized_questions = tf.keras.preprocessing.sequence.pad_sequences(self.tokenized_questions, maxlen=C_max_length, padding='post')\n","        self.tokenized_answers = tf.keras.preprocessing.sequence.pad_sequences(self.tokenized_answers, maxlen=C_max_length, padding='post')\n","        self.dataset = tf.data.Dataset.from_tensor_slices(({'XfIn': self.tokenized_questions,\n","                                                            'XfDeIn': self.tokenized_answers[:, :-1]},\n","                                                           self.tokenized_answers[:, 1:]))\n","        self.dataset = self.dataset.cache()\n","        self.dataset = self.dataset.shuffle(len(self.tokenized_questions))\n","        self.dataset = self.dataset.batch(C_batch_size)\n","        self.dataset = self.dataset.prefetch(tf.data.experimental.AUTOTUNE)\n","        return self.dataset, self.tokenizer\n","\n","\n","class ScaledDotProductAttention:\n","    \"\"\"Calculate the scaled dot product attention. The formula for calculating the scaled dot product attention is :\n","    ScaledDotProductAttention = softmax[(Q . K) / (D ** 0.5)] . V\n","    Mask is optional.\n","    \"\"\"\n","\n","    def __init__(self, q, k, v, mask):\n","        self.q = q\n","        self.k = k\n","        self.v = v\n","        self.mask = mask\n","\n","    def attention(self):\n","        \"\"\"Multiply Query and Key, scale it by square root of depth of key, softmax it and multiply by Value. \"\"\"\n","        qk = tf.matmul(self.q, self.k, transpose_b=True)\n","        scale_factor = tf.cast(tf.shape(self.k)[-1], tf.float32)\n","        logits = qk / tf.math.sqrt(scale_factor)\n","        if self.mask is not None:\n","            logits += (self.mask * -1e9)\n","        attention_weights = tf.nn.softmax(logits, axis=-1)\n","        sdpa = tf.matmul(attention_weights, self.v)\n","        return sdpa\n","\n","\n","class MultiHeadAttention(tf.keras.layers.Layer):\n","    \"\"\"Split Query Key Value into multiple heads so that the transformer will be able to attend to information at\n","    different positions at different representational spaces. \"\"\"\n","\n","    def __init__(self):\n","        super(MultiHeadAttention, self).__init__()\n","        self.depth = C_d_model // C_num_heads\n","        self.wq = tf.keras.layers.Dense(C_d_model)\n","        self.wk = tf.keras.layers.Dense(C_d_model)\n","        self.wv = tf.keras.layers.Dense(C_d_model)\n","        self.dense = tf.keras.layers.Dense(C_d_model)\n","\n","    def get_config(self):\n","        config = super(MultiHeadAttention, self).get_config()\n","        config.update({'num_heads': C_num_heads, 'd_model': C_d_model})\n","        return config\n","\n","    def split_heads(self, inputs, batch_size):\n","        inputs = tf.reshape(inputs, shape=(batch_size, -1, C_num_heads, self.depth))\n","        return tf.transpose(inputs, perm=[0, 2, 1, 3])\n","\n","    def call(self, inputs):\n","        \"\"\"Split query, key, value into multiple heads, calculate Scaled Dot Product Attention and concatenate those\n","        into a single attention. \"\"\"\n","        query, key, value, mask = inputs['query'], inputs['key'], inputs['value'], inputs['mask']\n","        batch_size = tf.shape(query)[0]\n","        query = self.wq(query)\n","        key = self.wk(key)\n","        value = self.wv(value)\n","\n","        query = self.split_heads(query, batch_size)\n","        key = self.split_heads(key, batch_size)\n","        value = self.split_heads(value, batch_size)\n","\n","        scaled_attention = ScaledDotProductAttention(query, key, value, mask).attention()\n","        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n","        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, C_d_model))\n","        outputs = self.dense(concat_attention)\n","\n","        return outputs\n","\n","\n","class PositionalEncoding(tf.keras.layers.Layer):\n","    \"\"\"Since the transformer is a non-recurrent model unlike RNNs and LSTMs, there has to be a positional encoding to\n","    denote the relative position of each word in the sequence. Otherwise, the transformer will effectively see a bag of\n","    wards with no information on the correlation of the words whatsoever. \"\"\"\n","\n","    def __init__(self):\n","        super(PositionalEncoding, self).__init__()\n","        self.pos_encoding = self.positional_encoding()\n","\n","    def get_config(self):\n","        config = super(PositionalEncoding, self).get_config()\n","        config.update({'position': C_actual_vocab_size, 'd_model': d_model})\n","        return config\n","\n","    def get_angles(self, pos, i):\n","        \"\"\"Defining the base formula for positional encoding. \"\"\"\n","        angles = 1 / tf.pow(10000, (2 * (i // 2) / tf.cast(C_d_model, tf.float32)))\n","        return pos * angles\n","\n","    def positional_encoding(self):\n","        \"\"\"Positional Encoding for each word - sine to words at even indices and cosine to words at odd indices. \"\"\"\n","        angle_rads = self.get_angles(pos = tf.cast(tf.range(C_actual_vocab_size)[:, tf.newaxis], dtype=tf.float32),\n","                                    i = tf.cast(tf.range(C_d_model)[tf.newaxis, :], dtype=tf.float32)\n","                                    )\n","        sines = tf.math.sin(angle_rads[:, 0::2])\n","        cosines = tf.math.cos(angle_rads[:, 1::2])\n","        pos_encoding = tf.concat([sines, cosines], axis=-1)\n","        pos_encoding = pos_encoding[tf.newaxis, ...]\n","        return pos_encoding\n","\n","    def call(self, inputs):\n","        return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]\n","\n","\n","class Encoder:\n","    \"\"\"Defining the encoder part of the transformer model. This will take in word embeddings and sums it up with the\n","    positional encoding. This aggregation is fed as an input to the encoder.\"\"\"\n","\n","    def __init__(self):\n","        \"\"\"Initializing the variables used in the class.\"\"\"\n","        self.en_inputs, self.el_inputs, self.en_outputs, self.el_outputs = None, None, None, None\n","        self.en_padding_mask, self.el_padding_mask = None, None\n","        self.en_embeddings, self.el_attention = None, None\n","\n","    def encoder_layer(self):\n","        self.el_inputs = tf.keras.Input(shape=(None, C_d_model))\n","        self.el_padding_mask = tf.keras.Input(shape=(1, 1, None))\n","        self.el_attention = MultiHeadAttention()({'query': self.el_inputs,\n","                                                  'key': self.el_inputs,\n","                                                  'value': self.el_inputs,\n","                                                  'mask': self.el_padding_mask,\n","                                                  })\n","        self.el_attention = tf.keras.layers.Dropout(C_dropout)(self.el_attention)\n","        self.el_attention = tf.keras.layers.LayerNormalization(epsilon=1e-6)(self.el_inputs + self.el_attention)\n","        self.el_outputs = tf.keras.layers.Dense(C_num_units, activation='relu')(self.el_attention)\n","        self.el_outputs = tf.keras.layers.Dense(C_d_model)(self.el_outputs)\n","        self.el_outputs = tf.keras.layers.Dropout(C_dropout)(self.el_outputs)\n","        self.el_outputs = tf.keras.layers.LayerNormalization(epsilon=1e-6)(self.el_attention + self.el_outputs)\n","        return tf.keras.Model(inputs=[self.el_inputs, self.el_padding_mask], outputs=self.el_outputs)\n","\n","    def encoder(self):\n","        self.en_inputs = tf.keras.Input(shape=(None,))\n","        self.en_padding_mask = tf.keras.Input(shape=(1, 1, None))\n","        self.en_embeddings = tf.keras.layers.Embedding(\n","            C_actual_vocab_size, C_d_model)(self.en_inputs)\n","        self.en_embeddings *= tf.math.sqrt(tf.cast(C_d_model, tf.float32))\n","        self.en_embeddings = PositionalEncoding()(self.en_embeddings)\n","        self.en_outputs = tf.keras.layers.Dropout(C_dropout)(self.en_embeddings)\n","        for i in range(C_num_layers):\n","            self.en_outputs = self.encoder_layer()([self.en_outputs, self.en_padding_mask])\n","        return tf.keras.Model(inputs=[self.en_inputs, self.en_padding_mask], outputs=self.en_outputs)\n","\n","\n","class Decoder:\n","    \"\"\"Defining the decoder part of the transformer. The decoder takes in the output of the encoder, processes it,\n","    decodes it and displays it in a human readable format.\"\"\"\n","    def __init__(self):\n","        \"\"\"Initializing the variables used in the class.\"\"\"\n","        self.de_padding_mask, self.dl_padding_mask, self.dl_en_outputs, self.de_en_outputs = None, None, None, None\n","        self.de_inputs, self.dl_inputs, self.de_outputs, self.dl_outputs = None, None, None, None\n","        self.de_embeddings, self.dl_attn1, self.dl_attn2 = None, None, None\n","        self.de_foresight_mask, self.dl_foresight_mask = None, None\n","\n","    def decoder_layer(self):\n","        self.dl_inputs = tf.keras.Input(shape=(None, C_d_model))\n","        self.dl_en_outputs = tf.keras.Input(shape=(None, C_d_model))\n","        self.dl_foresight_mask = tf.keras.Input(shape=(1, None, None))\n","        self.dl_padding_mask = tf.keras.Input(shape=(1, 1, None))\n","        self.dl_attn1 = MultiHeadAttention()(inputs={'query': self.dl_inputs,\n","                                                     'key': self.dl_inputs,\n","                                                     'value': self.dl_inputs,\n","                                                     'mask': self.dl_foresight_mask\n","                                                     })\n","        self.dl_attn1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)(self.dl_attn1 + self.dl_inputs)\n","        self.dl_attn2 = MultiHeadAttention()(inputs={'query': self.dl_attn1,\n","                                                     'key': self.dl_en_outputs,\n","                                                     'value': self.dl_en_outputs,\n","                                                     'mask': self.dl_padding_mask\n","                                                     })\n","        self.dl_attn2 = tf.keras.layers.Dropout(C_dropout)(self.dl_attn2)\n","        self.dl_attn2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)(self.dl_attn2 + self.dl_attn1)\n","        self.dl_outputs = tf.keras.layers.Dense(C_num_units, activation='relu')(self.dl_attn2)\n","        self.dl_outputs = tf.keras.layers.Dense(C_d_model)(self.dl_outputs)\n","        self.dl_outputs = tf.keras.layers.Dropout(C_dropout)(self.dl_outputs)\n","        self.dl_outputs = tf.keras.layers.LayerNormalization(epsilon=1e-6)(self.dl_outputs + self.dl_attn2)\n","        return tf.keras.Model(inputs=[self.dl_inputs, self.dl_en_outputs, self.dl_foresight_mask, self.dl_padding_mask],\n","                              outputs=self.dl_outputs)\n","\n","    def decoder(self):\n","        self.de_inputs = tf.keras.Input(shape=(None,))\n","        self.de_en_outputs = tf.keras.Input(shape=(None, C_d_model))\n","        self.de_foresight_mask = tf.keras.Input(shape=(1, None, None))\n","        self.de_padding_mask = tf.keras.Input(shape=(1, 1, None))\n","        self.de_embeddings = tf.keras.layers.Embedding(\n","            C_actual_vocab_size, C_d_model)(self.de_inputs)\n","        self.de_embeddings *= tf.math.sqrt(tf.cast(C_d_model, tf.float32))\n","        self.de_embeddings = PositionalEncoding()(self.de_embeddings)\n","        self.de_outputs = tf.keras.layers.Dropout(C_dropout)(self.de_embeddings)\n","        for i in range(C_num_layers):\n","            self.de_outputs = self.decoder_layer()(\n","                inputs=[self.de_outputs, self.de_en_outputs, self.de_foresight_mask, self.de_padding_mask])\n","        return tf.keras.Model(inputs=[self.de_inputs, self.de_en_outputs, self.de_foresight_mask, self.de_padding_mask],\n","                              outputs=self.de_outputs)\n","\n","\n","class Transformer:\n","    \"\"\"Building the transformer from component parts. \"\"\"\n","\n","    def __init__(self):\n","        self.xf_inputs, self.xf_de_inputs, self.xf_en_outputs, self.xf_de_outputs = None, None, None, None\n","        self.xf_en_padding_mask, self.xf_foresight_mask, self.xf_de_padding_mask = None, None, None\n","        self.pad_mask, self.foresight_mask, self.seq_len, self.pad_mask2 = None, None, None, None\n","        self.xf_outputs = None\n","\n","    def post_padding(self, x):\n","        self.pad_mask = tf.cast(tf.math.equal(x, 0), tf.float32)\n","        return self.pad_mask[:, tf.newaxis, tf.newaxis, :]\n","\n","    def block_foresight(self, x):\n","        self.seq_len = tf.shape(x)[1]\n","        self.foresight_mask = 1 - tf.linalg.band_part(tf.ones((self.seq_len, self.seq_len)), -1, 0)\n","        self.pad_mask2 = self.post_padding(x)\n","        return tf.maximum(self.foresight_mask, self.pad_mask2)\n","\n","    def transformer(self):\n","        self.xf_inputs = tf.keras.Input(shape=(None,), name='XfIn')\n","        self.xf_de_inputs = tf.keras.Input(shape=(None,), name='XfDeIn')\n","        self.xf_en_padding_mask = tf.keras.layers.Lambda(self.post_padding, output_shape=(1, 1, None), name='XfEnPM')(self.xf_inputs)\n","        self.xf_foresight_mask = tf.keras.layers.Lambda(self.block_foresight, output_shape=(1, None, None), name='XfFm')(self.xf_de_inputs)\n","        self.xf_de_padding_mask = tf.keras.layers.Lambda(self.post_padding, output_shape=(1, 1, None), name='XfDePm')(self.xf_inputs)\n","        self.xf_en_outputs = Encoder().encoder()(inputs=[self.xf_inputs, self.xf_en_padding_mask])\n","        self.xf_de_outputs = Decoder().decoder()(inputs=[self.xf_de_inputs, self.xf_en_outputs, self.xf_foresight_mask, self.xf_de_padding_mask])\n","        self.xf_outputs = tf.keras.layers.Dense(units=C_actual_vocab_size)(self.xf_de_outputs)\n","        return tf.keras.Model(inputs=[self.xf_inputs, self.xf_de_inputs], outputs=self.xf_outputs)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"S5NbAo2wX3Qb"},"source":["# Build and train the model"]},{"cell_type":"code","metadata":{"id":"MKDmCD_8U2-a"},"source":["# Necessary Helper Utilities\n","\n","def plot_generator():\n","    metrics = ['Root_Mean_Squared_Error', 'Loss']\n","    linetype = ['-', '--']\n","    fig, axes = plt.subplots(1, 2, figsize = (18, 5))\n","    epochs = range(len(history.history[metrics[0].lower()]))\n","    for i in range(len(metrics)):\n","        axes[i].plot(epochs, history.history[metrics[i].lower()], 'r'+ linetype[i], label = 'Training')\n","        axes[i].plot(epochs, history.history['val_' + metrics[i].lower()], 'b' + linetype[i] , label = 'Validation')\n","        axes[i].set_title(metrics[i], fontsize = 16)\n","        axes[i].grid(True)\n","        axes[i].set_xlabel('Epoch')\n","        axes[i].set_ylabel(metrics[i])\n","        axes[i].legend()\n","\n","def lrscheduler(epoch, lr):\n","  if epoch % C_Lr_Interval == C_Lr_Interval - 1:\n","    return lr * 0.1 \n","  else:\n","    return lr\n","\n","def cb_def():\n","    cb = [\n","          tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = C_Patience, restore_best_weights = True, min_delta = 0.0001),\n","          tf.keras.callbacks.LearningRateScheduler(lrscheduler)\n","         ]\n","    return cb"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TKazs2pRUApq"},"source":["data_util = DataUtilities()\n","dataset, tokenizer = data_util.tokenize()\n","print(len(tokenizer.word_counts))\n","jarvis = Transformer().transformer()\n","jarvis.compile(optimizer='sgd', loss='sparse_categorical_crossentropy', metrics=['sparse_categorical_accuracy'])\n","C_Patience = 5\n","C_Lr_Interval = 5\n","history = jarvis.fit(dataset, epochs=C_epochs, verbose=0, callbacks=cb_def())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZWvl6K8IVyTm"},"source":["txt = 'who is the father of tony'\n","txt = tf.expand_dims(C_start_token + tokenizer.encode(txt) + C_stop_token, axis=0)\n","outs = tf.expand_dims(C_start_token, axis=0)\n","predictions = jarvis([txt, outs])\n","predictions = predictions[:, -1:, :]\n","predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n","outs = tf.concat([outs, predicted_id], axis=-1)\n","output = tf.squeeze(outs, axis=0)\n","ansz = tokenizer.decode([i for i in output if i < tokenizer.vocab_size])\n","print(ansz)"],"execution_count":null,"outputs":[]}]}